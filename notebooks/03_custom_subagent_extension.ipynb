{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigator Agent: Custom Subagent Extension\n",
    "\n",
    "This notebook demonstrates how to create and integrate a custom subagent into the Navigator Agent workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from typing import Dict, Any, List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Import base subagent and state schemas\n",
    "from src.navigator_agent.agents.subagents.base import BaseSubagent\n",
    "from src.navigator_agent.schemas.agent_state import Solution, SolutionStatus"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SecurityComplianceSubagent(BaseSubagent):\n",
    "    \"\"\"Custom subagent for security and compliance analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: BaseLanguageModel):\n",
    "        super().__init__(llm)\n",
    "        self.name = \"security_compliance_agent\"\n",
    "    \n",
    "    def analyze_security(self, solutions: List[Solution]) -> List[Solution]:\n",
    "        \"\"\"\n",
    "        Enhance solutions with security and compliance insights.\n",
    "        \n",
    "        Args:\n",
    "            solutions: List of potential solutions\n",
    "        \n",
    "        Returns:\n",
    "            Enhanced solutions with security analysis\n",
    "        \"\"\"\n",
    "        enhanced_solutions = []\n",
    "        \n",
    "        for solution in solutions:\n",
    "            # Use LLM to analyze security aspects\n",
    "            security_prompt = f\"\"\"Analyze the security implications of this solution:\n",
    "{solution.content}\n",
    "\n",
    "Provide a detailed security assessment including:\n",
    "1. Potential vulnerabilities\n",
    "2. Compliance risks\n",
    "3. Recommended security improvements\n",
    "\"\"\"\n",
    "            \n",
    "            security_response = self.llm.invoke(security_prompt)\n",
    "            \n",
    "            # Update solution with security metrics\n",
    "            solution.evaluation_metrics['security_score'] = self._calculate_security_score(\n",
    "                solution, security_response.content\n",
    "            )\n",
    "            solution.metadata['security_analysis'] = security_response.content\n",
    "            \n",
    "            enhanced_solutions.append(solution)\n",
    "        \n",
    "        return enhanced_solutions\n",
    "    \n",
    "    def _calculate_security_score(self, solution: Solution, analysis: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate a security score based on the analysis.\n",
    "        \n",
    "        Args:\n",
    "            solution: Original solution\n",
    "            analysis: Security analysis text\n",
    "        \n",
    "        Returns:\n",
    "            Normalized security score\n",
    "        \"\"\"\n",
    "        # Simple heuristic for security scoring\n",
    "        risk_keywords = ['vulnerability', 'risk', 'threat', 'exploit']\n",
    "        security_keywords = ['encryption', 'authentication', 'secure', 'protection']\n",
    "        \n",
    "        risk_count = sum(analysis.lower().count(keyword) for keyword in risk_keywords)\n",
    "        security_count = sum(analysis.lower().count(keyword) for keyword in security_keywords)\n",
    "        \n",
    "        # Normalize score between 0 and 1\n",
    "        security_score = max(0, 1 - (risk_count * 0.2 - security_count * 0.1))\n",
    "        return round(security_score, 2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.navigator_agent.agents.navigator import NavigatorAgent\n",
    "\n",
    "# Initialize components\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.5)\n",
    "\n",
    "# Create custom security compliance subagent\n",
    "security_agent = SecurityComplianceSubagent(llm)\n",
    "\n",
    "# Initialize Navigator Agent\n",
    "navigator = NavigatorAgent(llm)\n",
    "\n",
    "# Inject custom subagent into workflow\n",
    "navigator.register_subagent(security_agent)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a problem with security focus\n",
    "problem_statement = \"Design a secure cloud-native banking application with multi-factor authentication\"\n",
    "\n",
    "# Process problem with custom security analysis\n",
    "workflow_result = navigator.process_input(problem_statement)\n",
    "\n",
    "# Display solutions with security insights\n",
    "print(\"Solutions with Security Analysis:\")\n",
    "for solution in workflow_result.possible_solutions:\n",
    "    print(f\"\\nSolution ID: {solution.id}\")\n",
    "    print(f\"Security Score: {solution.evaluation_metrics.get('security_score', 'N/A')}\")\n",
    "    print(\"Security Analysis:\")\n",
    "    print(solution.metadata.get('security_analysis', 'No detailed analysis'))"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
